{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Readme File**\n",
        "\n",
        "The project title - \"Weakly Supervised Sentiment Analysis for IMDb Movie Reviews: Logistic Regression and Pseudo-Labeling\", includes source code and this readme file that provides instructions\n",
        "on reproducing the experiments and generating plots. The source code contains the implementation of the sentiment analysis model for IMDb movie reviews, leveraging weak supervision, logistic regression, and pseudo-labeling.\n",
        "\n",
        "The readme file outlines is very simple to run in jupyter notebook (I run in Google Colab) in few steps and to set up the environment, preprocess the data, train the model, and evaluate its performance. It also provides guidance on generating relevant plots to visualize the results. The project aims to facilitate easy replication of the experiments and enable users to understand and utilize the sentiment analysis model effectively."
      ],
      "metadata": {
        "id": "NDCnnyXoWOHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download and Extract the Dataset**"
      ],
      "metadata": {
        "id": "5Zbj3q6qe4t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "dataset_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "download_path = \"aclImdb_v1.tar.gz\"\n",
        "extract_path = \"/content/aclImdb\"\n",
        "\n",
        "# Download the dataset\n",
        "os.system(f\"wget {dataset_url} -O {download_path}\")\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(download_path, \"r:gz\") as tar:\n",
        "    tar.extractall()"
      ],
      "metadata": {
        "id": "CIN2CFVVetX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Packages**"
      ],
      "metadata": {
        "id": "ko9E-XQRfCgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn nltk\n",
        "!pip install matplotlib scikit-learn\n",
        "%config NotebookApp.iopub_data_rate_limit = 10000000"
      ],
      "metadata": {
        "id": "_kBzvEz6TY7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "uVieAd4kJeL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from scipy.sparse import vstack\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9q9KzfKiU1n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "lkIJJux-J5Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(directory):\n",
        "    data = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                text = file.read()\n",
        "                data.append(text)\n",
        "    return data"
      ],
      "metadata": {
        "id": "wtgl_tqfJizb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_neg_data = load_data('/content/aclImdb/train/neg')\n",
        "train_pos_data = load_data('/content/aclImdb/train/pos')\n",
        "uns_data = load_data('/content/aclImdb/train/unsup')\n",
        "# Load test data\n",
        "test_neg_data = load_data('/content/aclImdb/test/neg')\n",
        "test_pos_data = load_data('/content/aclImdb/test/pos')"
      ],
      "metadata": {
        "id": "166xvSgtJi3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the Data**"
      ],
      "metadata": {
        "id": "mWK5y_-XfbA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "kWQ-FQ9gU3aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess training data\n",
        "train_neg_data = [preprocess(text) for text in train_neg_data]\n",
        "train_pos_data = [preprocess(text) for text in train_pos_data]\n",
        "uns_data = [preprocess(text) for text in uns_data]\n",
        "\n",
        "# Preprocess test data\n",
        "test_neg_data = [preprocess(text) for text in test_neg_data]\n",
        "test_pos_data = [preprocess(text) for text in test_pos_data]"
      ],
      "metadata": {
        "id": "GEPG9rDHVB8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_neg_data + train_pos_data\n",
        "test_data = test_neg_data + test_pos_data\n",
        "\n",
        "# Labels for training and testing data\n",
        "train_labels = [0] * len(train_neg_data) + [1] * len(train_pos_data)\n",
        "test_labels = [0] * len(test_neg_data) + [1] * len(test_pos_data)"
      ],
      "metadata": {
        "id": "j6KaZAy3gRiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_neg_data = len(train_neg_data)\n",
        "num_train_pos_data = len(train_pos_data)\n",
        "num_test_neg_data = len(test_neg_data)\n",
        "num_test_pos_data = len(test_pos_data)\n",
        "num_uns_data = len(uns_data)\n",
        "\n",
        "print(num_train_neg_data)\n",
        "print(num_train_pos_data)\n",
        "print(num_test_neg_data)\n",
        "print(num_test_pos_data)\n",
        "print(num_uns_data)"
      ],
      "metadata": {
        "id": "P7fGElxTmUl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data labels\n",
        "labels = ['Train', 'Test', 'Unlabeled']\n",
        "\n",
        "# Data counts\n",
        "train_neg_count = num_train_neg_data\n",
        "train_pos_count = num_train_pos_data\n",
        "test_neg_count = num_test_neg_data\n",
        "test_pos_count = num_test_pos_data\n",
        "uns_count = num_uns_data\n",
        "\n",
        "# Custom colors for each dataset\n",
        "colors = ['#FF69B4', '#8A2BE2', '#FFA500']\n",
        "\n",
        "# Plotting the stacked bar chart with custom colors\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x, [train_pos_count, test_pos_count, 0], color=colors[1], label='Positive')\n",
        "ax.bar(x, [train_neg_count, test_neg_count, 0], color=colors[0], label='Negative', bottom=[train_pos_count, test_pos_count, 0])\n",
        "ax.bar(x, [0, 0, uns_count], color=colors[2], label='Unlabeled')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Dataset')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Number of Data Points')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8UHR2wl5v1RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data labels\n",
        "labels = ['Train Negative', 'Train Positive', 'Test Negative', 'Test Positive', 'Unsupervised']\n",
        "\n",
        "# Data counts\n",
        "train_counts = [num_train_neg_data, num_train_pos_data, 0, 0, 0]\n",
        "test_counts = [0, 0, num_test_neg_data, num_test_pos_data, 0]\n",
        "uns_counts = [0, 0, 0, 0, num_uns_data]\n",
        "\n",
        "# Plotting the line plot\n",
        "plt.plot(labels, train_counts, label='Train')\n",
        "plt.plot(labels, test_counts, label='Test')\n",
        "plt.plot(labels, uns_counts, label='Unsupervised')\n",
        "\n",
        "plt.xlabel('Dataset')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Data Points')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lIwnaIXWvc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNm94P1-mUpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag-of-Words**"
      ],
      "metadata": {
        "id": "01dA2jbpgKBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_data + uns_data + test_data)\n",
        "\n",
        "train_features = vectorizer.transform(train_data)\n",
        "uns_features = vectorizer.transform(uns_data)\n",
        "test_features = vectorizer.transform(test_data)"
      ],
      "metadata": {
        "id": "krW_CQOJgHDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NB Model**"
      ],
      "metadata": {
        "id": "rpz6t7algd5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# Train initial models on labeled data\n",
        "nb_model = MultinomialNB().fit(train_features, train_labels)\n",
        "# Predict on unlabeled data\n",
        "threshold = 0.9  # Confidence threshold for pseudo-labeling\n",
        "\n",
        "nb_probs = nb_model.predict_proba(uns_features)\n",
        "nb_pseudo_labels = [np.argmax(prob) if max(prob) > threshold else -1 for prob in nb_probs]\n",
        "\n",
        "# Filter and prepare for retraining\n",
        "nb_pseudo_features = uns_features[np.array(nb_pseudo_labels) != -1]\n",
        "nb_pseudo_labels = np.array(nb_pseudo_labels)[np.array(nb_pseudo_labels) != -1]\n",
        "\n",
        "# Combine pseudo-labeled data with original training data\n",
        "combined_nb_features = vstack([train_features, nb_pseudo_features])\n",
        "combined_nb_labels = np.concatenate([train_labels, nb_pseudo_labels])\n",
        "\n",
        "# Retrain models\n",
        "nb_model_final = MultinomialNB().fit(combined_nb_features, combined_nb_labels)\n",
        "\n",
        "# Evaluate models\n",
        "nb_predictions = nb_model_final.predict(test_features)\n",
        "nb_accuracy = accuracy_score(test_labels, nb_predictions)\n",
        "nb_conf_matrix = confusion_matrix(test_labels, nb_predictions)\n",
        "\n",
        "print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n",
        "print(\"\\n\")\n",
        "print(\"Naive Bayes Confusion Matrix:\")\n",
        "print(nb_conf_matrix)\n",
        "print(\"\\n\")\n",
        "\n",
        "nb_precision = precision_score(test_labels, nb_predictions, average='macro')\n",
        "nb_recall = recall_score(test_labels, nb_predictions, average='macro')\n",
        "nb_f1 = f1_score(test_labels, nb_predictions, average='macro')\n",
        "print(\"NB Precision:\",nb_precision)\n",
        "print(\"NB Recall:\",nb_recall)\n",
        "print(\"NB F1:\",nb_f1)\n",
        "\n",
        "# Visualize the Confusion Matrix\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(nb_conf_matrix, annot=True, fmt='d', cmap='Oranges')\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIneKey2gdeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "nb_classification_report = classification_report(test_labels, nb_predictions)\n",
        "\n",
        "print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n",
        "print(\"\\n\")\n",
        "print(\"NB Classification Report:\")\n",
        "print(nb_classification_report)"
      ],
      "metadata": {
        "id": "pvf3rW0cVF-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LR Model**"
      ],
      "metadata": {
        "id": "WyJUnZfgjktp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# Train initial models on labeled data\n",
        "lr_model = LogisticRegression(max_iter=1000).fit(train_features, train_labels)\n",
        "\n",
        "# Predict on unlabeled data\n",
        "threshold = 0.9  # Confidence threshold for pseudo-labeling\n",
        "lr_probs = lr_model.predict_proba(uns_features)\n",
        "lr_pseudo_labels = [np.argmax(prob) if max(prob) > threshold else -1 for prob in lr_probs]\n",
        "\n",
        "# Filter and prepare for retraining\n",
        "lr_pseudo_features = uns_features[np.array(lr_pseudo_labels) != -1]\n",
        "lr_pseudo_labels = np.array(lr_pseudo_labels)[np.array(lr_pseudo_labels) != -1]\n",
        "\n",
        "# Combine pseudo-labeled data with original training data\n",
        "combined_lr_features = vstack([train_features, lr_pseudo_features])\n",
        "combined_lr_labels = np.concatenate([train_labels, lr_pseudo_labels])\n",
        "\n",
        "# Retrain models\n",
        "lr_model_final = LogisticRegression(max_iter=1000).fit(combined_lr_features, combined_lr_labels)\n",
        "\n",
        "# Evaluate models\n",
        "lr_predictions = lr_model_final.predict(test_features)\n",
        "lr_accuracy = accuracy_score(test_labels, lr_predictions)\n",
        "lr_conf_matrix = confusion_matrix(test_labels, lr_predictions)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "lr_precision = precision_score(test_labels, lr_predictions, average='macro')\n",
        "lr_recall = recall_score(test_labels, lr_predictions, average='macro')\n",
        "lr_f1 = f1_score(test_labels, lr_predictions, average='macro')\n",
        "print(\"LR Precision:\",nb_precision)\n",
        "print(\"LR Recall:\",nb_recall)\n",
        "print(\"LR F1:\",nb_f1)\n",
        "\n",
        "print(\"Logistic Regression Confusion Matrix:\")\n",
        "print(lr_conf_matrix)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualize the Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(lr_conf_matrix, annot=True, fmt='d', cmap='YlGn')\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zPNFlVflbt_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "lr_classification_report = classification_report(test_labels, lr_predictions)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
        "print(\"\\n\")\n",
        "print(\"LR Classification Report:\")\n",
        "print(lr_classification_report)"
      ],
      "metadata": {
        "id": "N8iyi221TjrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Models**"
      ],
      "metadata": {
        "id": "rAr6p9Sfkx6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Results\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "nb_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
        "lr_scores = [lr_accuracy, lr_precision, lr_recall, lr_f1]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "x = range(len(metrics))\n",
        "plt.bar(x, nb_scores, width=0.2, label='NB')\n",
        "plt.bar([i + 0.2 for i in x], lr_scores, width=0.2, label='LR')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Scores')\n",
        "plt.xticks([i for i in x], metrics)\n",
        "plt.title('Comparison of Sentiment Analysis Approaches')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "lr_probs = lr_model_final.predict_proba(test_features)[:, 1]\n",
        "lr_fpr, lr_tpr, _ = roc_curve(test_labels, lr_probs)\n",
        "lr_auc = roc_auc_score(test_labels, lr_probs)\n",
        "\n",
        "nb_probs = nb_model_final.predict_proba(test_features)[:, 1]\n",
        "nb_fpr, nb_tpr, _ = roc_curve(test_labels, nb_probs)\n",
        "nb_auc = roc_auc_score(test_labels, nb_probs)\n",
        "\n",
        "plt.plot(lr_fpr, lr_tpr, label='LR (AUC = {:.2f})'.format(lr_auc))\n",
        "plt.plot(nb_fpr, nb_tpr, label='NB (AUC = {:.2f})'.format(nb_auc))\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='blue')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - LR vs NB')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "labels = ['Naive Bayes', 'Logistic Regression']\n",
        "accuracies = [nb_accuracy, lr_accuracy]\n",
        "colors = ['#66C2A5', '#FC8D62']\n",
        "plt.pie(accuracies, labels=labels, autopct='%1.1f%%', colors=colors)\n",
        "plt.title('Accuracy Comparison: NB vs LR')\n",
        "plt.show()\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "BjLvCN8Ak5sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gE_WZ2vqCyQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rr30vgEmCbyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}